# -*- coding: utf-8 -*-
"""Wiki_MultiLabel_Prediction_SolarEvents.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hMF9i_5B7hvKmiJlB8aFmD-0q72ztZMx
"""

# Installing BeautifulSoap to scrape information from web pages
!pip install pandas beautifulsoup4 lxml requests

import requests
from bs4 import BeautifulSoup
import pandas as pd

def scrape_website(url, event_name, event_description):
    try:
        # Sending HTTP request
        sentence = " "
        response = requests.get(url)
        response.raise_for_status()  # Raising exception for HTTP errors
        # Parsing content
        soup = BeautifulSoup(response.text, 'html.parser')
        title = soup.find('title').text if soup.find('title') else event_name
        paragraphs = [p.text for p in soup.find_all('p')]
        if len(paragraphs) == 0:
          sentence += event_description
        else:
          for para in paragraphs:
            sentence += para

        return title, sentence
    except requests.RequestException as e:
        return event_name, event_description

def main(urls, event_names, event_descriptions):
    # List to hold all data and paragraphs
    all_data = []
    for url, event_name, event_description in zip(urls, event_names, event_descriptions):
        title, paragraphs = scrape_website(url, event_name, event_description)
        all_data.append([title, paragraphs, url])

    # Creating DataFrame for all the data
    df = pd.DataFrame(all_data, columns=['Event', 'Event description','Link to additional info'])

    return df

from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import nltk

# Text preprocessing of 'Event description' column

nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    # Converting to lower case
    text = text.lower()

    # Removing stopwords and stem the words
    stemmer = PorterStemmer()
    text = ' '.join([stemmer.stem(word) for word in text.split() if word not in stop_words])
    return text

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.feature_extraction.text import TfidfVectorizer


# Load data to model building
df = pd.read_csv('/content/events_data.csv')

# 'Tags' column contains the labels separated by a comma
df['Tags'] = df['Tags'].apply(lambda x: x.split(','))

urls = df['Link to additional info']
event_names = df['Event name']
event_descriptions = df['Event description']

df_input = main(urls, event_names, event_descriptions)
print(df_input)

df_input['Event description'] = df_input['Event description'].apply(preprocess_text)

# Vectorization of the preprocessed text
vectorizer = TfidfVectorizer(max_features=1000)
X = vectorizer.fit_transform(df_input['Event description'].values).toarray()

# Preparing labels
mlb = MultiLabelBinarizer()
y = mlb.fit_transform(df['Tags'])

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

model = Sequential([
    Dense(1024, input_dim=X_train.shape[1], activation='relu'),
    Dropout(0.6),
    Dense(512, activation='relu'),
    Dropout(0.6),
    Dense(y_train.shape[1], activation='sigmoid')
])

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

history = model.fit(X_train, y_train, epochs=20, batch_size=64, validation_data=(X_test, y_test))

def scrape_website1(url):
    try:
        # Sending HTTP request
        sentence = " "
        response = requests.get(url)
        response.raise_for_status()  # Raising exception for HTTP errors
        # Parsing content
        soup = BeautifulSoup(response.text, 'html.parser')
        title = soup.find('title').text if soup.find('title') else "no title"
        paragraphs = [p.text for p in soup.find_all('p')]
        return title, paragraphs
    except requests.RequestException as e:
        return "Error", []

def main1(urls):
    # List to hold all data and paragraphs
    all_data = []
    for url in urls:
        title, paragraphs = scrape_website1(url)
        all_data.append([title, paragraphs, url])

    # Creating DataFrame for all the data
    df = pd.DataFrame(all_data, columns=['Event', 'Event description','Link to additional info'])

    return df

# Input URL list from "relevant wikipedia pages"
urls = [
    "https://en.wikipedia.org/wiki/Perovskite_solar_cell",
    "https://en.wikipedia.org/wiki/Solar_cell",
    "https://en.wikipedia.org/wiki/Semiconductor"
]

# Calling the main function and printing the DataFrame
df_wiki = main1(urls)
df_wiki.to_csv("/content/scraped_data.csv", index=False)
print(df_wiki)

# Loading and preparing data
data = pd.read_csv('/content/scraped_data.csv')
data['Event description'] = data['Event description'].apply(preprocess_text)
X_new = vectorizer.transform(data['Event description']).toarray()
print(X_new)

# Predicting tags
y_new_pred = model.predict(X_new)
y_new_pred = (y_new_pred > 0.5)  # Declaring threshold
new_tags = mlb.inverse_transform(y_new_pred)

# Adding predictions to DataFrame
data['Predicted Tags'] = [', '.join(tags) if tags else 'No Tags' for tags in new_tags]


data.to_csv('/content/final_predicted_solar_event_data.csv', index=False)
print(data[['Event description', 'Predicted Tags']])

"""Visualizations"""

!pip install matplotlib seaborn pandas

from collections import Counter
import matplotlib.pyplot as plt
import ast
import seaborn as sns


tag_data = mlb.fit_transform(data['Predicted Tags'])
tag_df = pd.DataFrame(tag_data, columns=mlb.classes_)

# Compute the correlation matrix
corr = tag_df.corr()

# Generate a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(corr, annot=True, fmt=".2f", cmap='coolwarm')
plt.title('Heatmap of Tag Co-occurrence')
plt.show()



# Load data
df = pd.read_csv('/content/final_predicted_solar_event_data.csv')

# Count labels
label_counts = Counter([label for label in df['Predicted Tags']])

# Plotting label frequency
labels, counts = zip(*label_counts.items())
plt.figure(figsize=(10, 8))
plt.bar(labels, counts, color='blue')
plt.xlabel('Labels', fontsize=14)
plt.ylabel('Frequency', fontsize=14)
plt.xticks(rotation=90)
plt.title('Frequency of Each Label', fontsize=16)
plt.show()